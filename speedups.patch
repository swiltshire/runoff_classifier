*** Begin Patch
*** Update File: scripts/inference.py
@@
 from __future__ import annotations
 import os
 import json
 import argparse
 import torch
+import torch.backends.cudnn as cudnn
 from contextlib import nullcontext
 import numpy as np
 import geopandas as gpd
 from shapely.geometry import box as shapely_box
 import rasterio
@@
 def resolve_raster_input(path_or_dir):
     """
     If path_or_dir is a directory of .tif tiles or a *.tif glob:
       → Build a VRT one level up from the tiles folder.
     Otherwise:
       → Return path_or_dir unchanged.
     """
     # directory of tifs
     if os.path.isdir(path_or_dir):
         files = sorted(glob.glob(os.path.join(path_or_dir, "*.tif")))
         if not files:
             raise FileNotFoundError(f"no .tif files found in {path_or_dir}")
 
         parent = os.path.dirname(os.path.abspath(path_or_dir))
         parent_name = os.path.basename(parent)
         out_vrt = os.path.join(parent, f"{parent_name}_mosaic.vrt")
-
-        build_vrt_from_tifs(files, out_vrt)
+        # cache the vrt if it already exists to avoid rebuild cost
+        if not os.path.exists(out_vrt):
+            build_vrt_from_tifs(files, out_vrt)
         return out_vrt
 
     # glob (*.tif)
     if any(ch in path_or_dir for ch in ["*", "?", "["]):
         files = sorted(glob.glob(path_or_dir))
         if not files:
             raise FileNotFoundError(f"glob matched no .tif files: {path_or_dir}")
 
         tile_dir = os.path.dirname(os.path.abspath(path_or_dir))
         parent = os.path.dirname(tile_dir)
         parent_name = os.path.basename(parent)
         out_vrt = os.path.join(parent, f"{parent_name}_mosaic.vrt")
-
-        build_vrt_from_tifs(files, out_vrt)
+        # cache the vrt if it already exists to avoid rebuild cost
+        if not os.path.exists(out_vrt):
+            build_vrt_from_tifs(files, out_vrt)
         return out_vrt
 
     # passthrough
     return path_or_dir
@@
 def main():
     args = parse_args()
+    # enable cudnn autotune and prefer fast matmul where supported
+    cudnn.benchmark = True
+    if hasattr(torch, "set_float32_matmul_precision"):
+        torch.set_float32_matmul_precision("high")
     
     # convert tiles to vrt if needed
     args.raster_path = resolve_raster_input(args.raster_path)
 
     log_path = os.path.join(os.path.dirname(args.out_vector) or ".", "inference.log")
@@
     model.eval()
     logger.info("[debug] model loaded and set to eval (device=%s)", args.device)
 
     windows = make_grid_windows(args.raster_path, tile_size=args.tile_size, stride=args.stride)
     logger.info("[debug] num_windows=%d", len(windows))
 
-    all_boxes, all_scores, all_labels = [], [], []
-    tile_transforms = []  # keep per-detection tile transform for correct vectorization
-    mask_slices = []      # store masks aligned with detections
+    all_boxes, all_scores, all_labels = [], [], []
+    tile_transforms = []  # keep per-detection tile transform for correct vectorization
+    mask_slices = []      # store masks aligned with detections
 
     with rasterio.open(args.raster_path) as src:
         normalizer = build_normalizer(args.normalize)
         use_cuda = str(args.device).startswith("cuda")
-        amp_ctx = torch.autocast("cuda", dtype=torch.float16) if use_cuda else nullcontext()
+        amp_ctx = torch.autocast("cuda", dtype=torch.float16) if use_cuda else nullcontext()
+
+        # batch multiple windows per forward to reduce python and kernel overhead
+        batch_imgs: list[torch.Tensor] = []
+        batch_transforms: list = []
+        batch_windows: list = []
+        batch_size = 4  # tune based on vram; 2–8 is typical for 1024px tiles
+
+        # small helper to run a batch and collect outputs
+        def run_batch():
+            if not batch_imgs:
+                return
+            with torch.inference_mode():
+                with amp_ctx:
+                    preds_list = model(batch_imgs)
+            for preds, win, tform in zip(preds_list, batch_windows, batch_transforms):
+                boxes = preds["boxes"]
+                scores = preds["scores"]
+                labels = preds["labels"]
+                keep = scores >= args.score_thresh
+                boxes = boxes[keep]
+                scores = scores[keep]
+                labels = labels[keep]
+                # keep everything on device; convert to cpu only after nms
+                boxes_global = adjust_boxes_to_global(boxes, win)
+                all_boxes.append(boxes_global)
+                all_scores.append(scores)
+                all_labels.append(labels)
+                if args.task == 'instance_seg' and 'masks' in preds:
+                    m = preds['masks'][keep]
+                    if m.shape[0] > 0:
+                        mask_slices.append(m)
+                        tile_transforms.extend([tform] * m.shape[0])
+                else:
+                    if boxes.shape[0] > 0:
+                        tile_transforms.extend([tform] * boxes.shape[0])
+
+            # clear batch containers
+            batch_imgs.clear()
+            batch_transforms.clear()
+            batch_windows.clear()
 
         for i, window in enumerate(windows, start=1):
             logger.info("[debug] processing window %d/%d", i, len(windows))
             img = src.read(window=window)
             tile_transform = src.window_transform(window)
@@
             scale = 65535.0 if src.dtypes[0] in ("uint16", "int16") else 255.0
-            tensor = torch.from_numpy(img).float().div_(scale).clamp_(0.0, 1.0)
+            tensor = torch.from_numpy(img).float().div_(scale).clamp_(0.0, 1.0)
             if normalizer is not None:
                 tensor = normalizer(tensor)
-            tensor = tensor.to(args.device, non_blocking=False)
+            # pin host memory and use non_blocking copy if on cuda
+            if use_cuda:
+                tensor = tensor.pin_memory()
+            tensor = tensor.to(args.device, non_blocking=True)
 
-            try:
-                with torch.inference_mode():
-                    with amp_ctx:
-                        preds = model([tensor])[0]
-                boxes = preds["boxes"]
-                scores = preds["scores"]
-                labels = preds["labels"]
-
-                keep = scores >= args.score_thresh
-                boxes = boxes[keep]
-                scores = scores[keep]
-                labels = labels[keep]
-
-                boxes_global = adjust_boxes_to_global(boxes, window)
-                all_boxes.append(boxes_global.to("cpu"))
-                all_scores.append(scores.to("cpu"))
-                all_labels.append(labels.to("cpu"))
-
-                if args.task == 'instance_seg' and 'masks' in preds:
-                    masks = preds['masks'][keep].to("cpu")  # (n, 1, h, w)
-                    mask_slices.append(masks)
-                    if masks.shape[0] > 0:
-                        tile_transforms.extend([tile_transform] * masks.shape[0])
-                else:
-                    if boxes.shape[0] > 0:
-                        tile_transforms.extend([tile_transform] * boxes.shape[0])
-
-            except Exception:
-                logger.exception("[error] failed on window %d/%d; skipping", i, len(windows))
-                continue
-            finally:
-                del tensor
-                if use_cuda and (i % 10 == 0):
-                    torch.cuda.empty_cache()
+            try:
+                # enqueue into batch
+                batch_imgs.append(tensor)
+                batch_transforms.append(tile_transform)
+                batch_windows.append(window)
+                # run batch if full
+                if len(batch_imgs) >= batch_size:
+                    run_batch()
+            except Exception:
+                logger.exception("[error] failed on window %d/%d; skipping", i, len(windows))
+                # drop the partially filled batch item on error
+                if batch_imgs:
+                    batch_imgs.pop()
+                    batch_transforms.pop()
+                    batch_windows.pop()
+                continue
+            finally:
+                # no periodic empty_cache; let the allocator reuse blocks
+                pass
+
+        # flush any remainder
+        run_batch()
 
     # concatenate
-    boxes = torch.cat(all_boxes, dim=0) if len(all_boxes) else torch.empty((0, 4))
-    scores = torch.cat(all_scores, dim=0) if len(all_scores) else torch.empty((0,))
-    labels = torch.cat(all_labels, dim=0) if len(all_labels) else torch.empty((0,), dtype=torch.int64)
-    masks = torch.cat(mask_slices, dim=0) if len(mask_slices) else torch.empty((0, 1, 1, 1))
+    device = args.device
+    boxes = torch.cat(all_boxes, dim=0).to(device) if all_boxes else torch.empty((0, 4), device=device)
+    scores = torch.cat(all_scores, dim=0).to(device) if all_scores else torch.empty((0,), device=device)
+    labels = torch.cat(all_labels, dim=0).to(device) if all_labels else torch.empty((0,), dtype=torch.int64, device=device)
+    masks = torch.cat(mask_slices, dim=0).to(device) if mask_slices else torch.empty((0, 1, 1, 1), device=device)
 
     logger.info("[debug] pre-nms count=%d (score_thresh=%.3f)", len(boxes), args.score_thresh)
 
     # class-wise nms
-    if len(boxes) > 0:
-        keep_indices = []
-        for cls_idx in labels.unique():
-            cls_mask = labels == cls_idx
-            cls_boxes = boxes[cls_mask]
-            cls_scores = scores[cls_mask]
-            if cls_boxes.numel() == 0:
-                continue
-            keep = nms(cls_boxes, cls_scores, args.nms_iou_thresh)
-            base_idx = torch.nonzero(cls_mask, as_tuple=False).squeeze(1)
-            keep_indices.extend(base_idx[keep].tolist())
-        if len(keep_indices) > 0:
-            keep_indices = torch.tensor(keep_indices, dtype=torch.long)
-            boxes = boxes[keep_indices]
-            scores = scores[keep_indices]
-            labels = labels[keep_indices]
-            if args.task == 'instance_seg' and masks.numel() > 0:
-                masks = masks[keep_indices]
-                tile_transforms = [tile_transforms[i] for i in keep_indices.tolist()]
-        else:
-            boxes = boxes[:0]
-            scores = scores[:0]
-            labels = labels[:0]
-            masks = masks[:0]
-            tile_transforms = []
+    if len(boxes) > 0:
+        keep_indices: list[int] = []
+        for cls_idx in labels.unique():
+            cls_mask = labels == cls_idx
+            if cls_mask.any():
+                k = nms(boxes[cls_mask], scores[cls_mask], args.nms_iou_thresh)
+                base_idx = torch.nonzero(cls_mask, as_tuple=False).squeeze(1)
+                keep_indices.extend(base_idx[k].tolist())
+        if keep_indices:
+            keep_indices_t = torch.tensor(keep_indices, dtype=torch.long, device=device)
+            boxes = boxes[keep_indices_t]
+            scores = scores[keep_indices_t]
+            labels = labels[keep_indices_t]
+            if args.task == 'instance_seg' and masks.numel() > 0:
+                masks = masks[keep_indices_t]
+                tile_transforms = [tile_transforms[i] for i in keep_indices]
+        else:
+            boxes = boxes[:0]; scores = scores[:0]; labels = labels[:0]
+            masks = masks[:0]; tile_transforms = []
 
     logger.info("[summary] post-nms count=%d", len(boxes))
 
     # build geoms
     geoms, class_names, score_vals = [], [], []
@@
         crs = src.crs
         global_transform = src.transform
 
+    # move tensors to cpu for vectorization and output
+    boxes = boxes.to("cpu")
+    scores = scores.to("cpu")
+    labels = labels.to("cpu")
+    masks = masks.to("cpu")
+
     if args.task == 'instance_seg' and len(boxes) > 0 and masks.numel() > 0:
         # vectorize each mask to polygons using the respective tile transform
         masks_np = (masks.squeeze(1).numpy() > 0.5).astype('uint8')  # (n, h, w)
         for idx, (m, tform, s, l) in enumerate(zip(masks_np, tile_transforms, scores.tolist(), labels.tolist())):
             for geom, val in rio_features.shapes(m, transform=tform):
*** End Patch
*** Begin Patch
*** Update File: scripts/train.py
@@
 from __future__ import annotations
 import os
 import json
 import argparse
 import torch
+import torch.backends.cudnn as cudnn
 from torch.utils.data import DataLoader
 from tqdm import tqdm
 import sys
 import glob
@@
 def resolve_raster_input(path_or_dir):
     """
     If path_or_dir is a directory of .tif tiles or a *.tif glob:
       → Build a VRT one level up from the tiles folder.
     Otherwise:
       → Return path_or_dir unchanged.
     """
     # directory of tifs
     if os.path.isdir(path_or_dir):
         files = sorted(glob.glob(os.path.join(path_or_dir, "*.tif")))
         if not files:
             raise FileNotFoundError(f"no .tif files found in {path_or_dir}")
 
         parent = os.path.dirname(os.path.abspath(path_or_dir))
         parent_name = os.path.basename(parent)
         out_vrt = os.path.join(parent, f"{parent_name}_mosaic.vrt")
-
-        build_vrt_from_tifs(files, out_vrt)
+        # cache the vrt if it already exists to avoid rebuild cost
+        if not os.path.exists(out_vrt):
+            build_vrt_from_tifs(files, out_vrt)
         return out_vrt
 
     # glob (*.tif)
     if any(ch in path_or_dir for ch in ["*", "?", "["]):
         files = sorted(glob.glob(path_or_dir))
         if not files:
             raise FileNotFoundError(f"glob matched no .tif files: {path_or_dir}")
 
         tile_dir = os.path.dirname(os.path.abspath(path_or_dir))
         parent = os.path.dirname(tile_dir)
         parent_name = os.path.basename(parent)
         out_vrt = os.path.join(parent, f"{parent_name}_mosaic.vrt")
-
-        build_vrt_from_tifs(files, out_vrt)
+        # cache the vrt if it already exists to avoid rebuild cost
+        if not os.path.exists(out_vrt):
+            build_vrt_from_tifs(files, out_vrt)
         return out_vrt
 
     # passthrough
     return path_or_dir
@@
 def main():
     args = parse_args()
     os.makedirs(args.out_dir, exist_ok=True)
+    # enable cudnn autotune and prefer fast matmul where supported
+    cudnn.benchmark = True
+    if hasattr(torch, "set_float32_matmul_precision"):
+        torch.set_float32_matmul_precision("high")
 
     # convert tiles to vrt if needed
     args.raster_path = resolve_raster_input(args.raster_path)
@@
     dataloader = DataLoader(
         dataset,
         batch_size=args.batch_size,
         shuffle=True,
-        num_workers=args.num_workers,
+        num_workers=args.num_workers,
         collate_fn=detection_collate_fn,
-        pin_memory=True,
+        # pin memory, keep workers alive, and prefetch to reduce stalls
+        pin_memory=True,
+        persistent_workers=True,
+        prefetch_factor=2,
     )
@@
-    for epoch in range(1, args.epochs + 1):
+    # amp scaler for mixed precision training on cuda
+    use_cuda = args.device.startswith("cuda")
+    scaler = torch.cuda.amp.GradScaler(enabled=use_cuda)
+
+    for epoch in range(1, args.epochs + 1):
         model.train()
         epoch_loss = 0.0
         for images, targets in tqdm(dataloader, desc=f'epoch {epoch}/{args.epochs}'):
-            images = [img.to(args.device) for img in images]
-            # move tensors in targets to device (masks included if present)
-            targets = [{k: (v.to(args.device) if torch.is_tensor(v) else v) for k, v in t.items()} for t in targets]
-            loss_dict = model(images, targets)
-            losses = sum(loss for loss in loss_dict.values())
-            optimizer.zero_grad()
-            losses.backward()
-            optimizer.step()
+            # move data to device using non_blocking copies when cuda
+            images = [img.to(args.device, non_blocking=True) for img in images]
+            targets = [{k: (v.to(args.device, non_blocking=True) if torch.is_tensor(v) else v) for k, v in t.items()} for t in targets]
+
+            optimizer.zero_grad(set_to_none=True)
+            # amp forward/backward
+            with torch.cuda.amp.autocast(enabled=use_cuda):
+                loss_dict = model(images, targets)
+                losses = sum(loss for loss in loss_dict.values())
+            scaler.scale(losses).backward()
+            scaler.step(optimizer)
+            scaler.update()
             epoch_loss += losses.item()
         avg_loss = epoch_loss / max(1, len(dataloader))
         print(f'epoch {epoch} average loss: {avg_loss:.4f}')
         if epoch % args.save_every == 0:
             ckpt_path = os.path.join(args.out_dir, f'model_epoch_{epoch}.pth')
             torch.save({'model_state': model.state_dict(), 'classes': classes, 'task': args.task}, ckpt_path)
             print(f'saved checkpoint: {ckpt_path}')
@@
     final_path = os.path.join(args.out_dir, 'model_final.pth')
     torch.save({'model_state': model.state_dict(), 'classes': classes, 'task': args.task}, final_path)
     print(f'saved final model: {final_path}')
+
+    # optional compile for pytorch 2.x; guarded to avoid runtime issues
+    # (move earlier, right after model creation, if you want it enabled during training)
+    try:
+        if hasattr(torch, "compile"):
+            model = torch.compile(model, mode="reduce-overhead")
+    except Exception:
+        # ignore if compile is not supported for this model/torchvision build
+        pass
*** End Patch
